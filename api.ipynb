{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data collection for software devloper row using api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apify-client in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: apify-shared>=1.1.2 in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from apify-client) (1.2.1)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from apify-client) (0.27.0)\n",
      "Requirement already satisfied: more_itertools>=10.0.0 in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from apify-client) (10.3.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->apify-client) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\vbhav\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->apify-client) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install apify-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection for Software Developer using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 320 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_SD.csv\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SD.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection for UX Designer using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 17 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_UX.csv\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_UX.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection for Applications Developer using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 280 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_AD.csv\n"
     ]
    }
   ],
   "source": [
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_AD.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection for CRM Technical Developer using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 260 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_CTD.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_CTD.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection for Database Developer using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 100 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_DD.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_DD.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection for Mobile Applications Developer using API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 15 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_MAD.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_MAD.csv'  \n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data collection from Software Engineer using api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 260 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_SE.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SE.csv' \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection for Network Security Engineer using API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 17 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_NSE.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_NSE.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection for Software Quality Assurance (QA) / Testing using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 12 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_SQA.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SQA.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection Systems Security Administrator for using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 13 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_SSA.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SSA.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection  for Technical Support using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 80 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_TS.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_TS.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection  for  Web Developer using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 120 job listings to C://CUB//sem2//ml//proj//cn//datasets//job_listings_wd.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from apify_client import ApifyClient\n",
    "import csv\n",
    "\n",
    "client = ApifyClient(\"apify_api_idJVhUenaiyNjqMU7nYPsf3x0i2AQb3dfX2I\")\n",
    "\n",
    "run = client.task(\"Ze6eUqoFHTeQ9N6gL\").call()\n",
    "\n",
    "csv_file = r'C://CUB//sem2//ml//proj//cn//datasets//job_listings_wd.csv'  \n",
    "fieldnames = set()\n",
    "\n",
    "all_items = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    company = item.pop('company', {})\n",
    "    for key, value in company.items():\n",
    "        if isinstance(value, list):\n",
    "            value = '; '.join(value)\n",
    "        item[f'company_{key}'] = value\n",
    "    \n",
    "    for field in ['benefits', 'requirements', 'locations']:\n",
    "        if field in item and isinstance(item[field], list):\n",
    "            item[field] = '; '.join(item[field])\n",
    "    \n",
    "    fieldnames.update(item.keys())\n",
    "    all_items.append(item)\n",
    "\n",
    "fieldnames = sorted(fieldnames)\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_items)\n",
    "\n",
    "print(f'Successfully saved {len(all_items)} job listings to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_AD.csv' with new title 'Applications Developer'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_CTD.csv' with new title 'CRM Technical Developer'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_DD.csv' with new title 'Database Developer'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_MAD.csv' with new title 'Mobile Applications Developer'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_NSE.csv' with new title 'Network Security Engineer'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SD.csv' with new title 'Software Developer'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SE.csv' with new title 'Software Engineer'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SQA.csv' with new title 'Software Quality Assurance (QA) / Testing'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SSA.csv' with new title 'Systems Security Administrator'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_TS.csv' with new title 'Technical Support'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_UX.csv' with new title 'UX Designer'\n",
      "Updated 'C://CUB//sem2//ml//proj//cn//datasets//job_listings_WD.csv' with new title 'Web Developer'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "files_and_titles = {\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_AD.csv': 'Applications Developer',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_CTD.csv': 'CRM Technical Developer',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_DD.csv': 'Database Developer',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_MAD.csv': 'Mobile Applications Developer',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_NSE.csv': 'Network Security Engineer',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SD.csv': 'Software Developer',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SE.csv': 'Software Engineer',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SQA.csv': 'Software Quality Assurance (QA) / Testing',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SSA.csv': 'Systems Security Administrator',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_TS.csv': 'Technical Support',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_UX.csv': 'UX Designer',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_WD.csv': 'Web Developer'\n",
    "}\n",
    "\n",
    "for file_path, new_title in files_and_titles.items():\n",
    "    if os.path.exists(file_path):\n",
    "        data = pd.read_csv(file_path)\n",
    "        data['title'] = new_title\n",
    "        data.to_csv(file_path, index=False)\n",
    "        print(f\"Updated '{file_path}' with new title '{new_title}'\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_AD.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_CTD.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_DD.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_MAD.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_NSE.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_SD.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_SE.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_SQA.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_SSA.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_TS.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_UX.csv\n",
      "Added data from C://CUB//sem2//ml//proj//cn//datasets//job_listings_WD.csv\n",
      "All files have been combined into C://CUB//sem2//ml//proj//cn//datasets//combined_job_listings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_paths = [\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_AD.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_CTD.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_DD.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_MAD.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_NSE.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SD.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SE.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SQA.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_SSA.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_TS.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_UX.csv',\n",
    "    'C://CUB//sem2//ml//proj//cn//datasets//job_listings_WD.csv'\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "        print(f\"Added data from {file_path}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "if dfs:\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    combined_file_path = 'C://CUB//sem2//ml//proj//cn//datasets//combined_job_listings.csv'\n",
    "    combined_df.to_csv(combined_file_path, index=False)\n",
    "    print(f\"All files have been combined into {combined_file_path}\")\n",
    "else:\n",
    "    print(\"No files were combined, as no data frames were loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C://Users//vbhav//Downloads//prediction-data (1).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[0;32m      4\u001b[0m combined_job_listings \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC://CUB//sem2//ml//proj//cn//datasets//combined_job_listings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m prediction_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC://Users//vbhav//Downloads//prediction-data (1).csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Merge the datasets on the specified columns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(combined_job_listings, prediction_data, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuggestedJobRole\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    729\u001b[0m     path_or_buf,\n\u001b[0;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    734\u001b[0m )\n\u001b[0;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:432\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    430\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mopen(\n\u001b[0;32m    431\u001b[0m         filepath_or_buffer, mode\u001b[38;5;241m=\u001b[39mfsspec_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[1;32m--> 432\u001b[0m     )\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(err_types_to_retry_with_anon):\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\fsspec\\core.py:147\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    The OpenFile object should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    instances persisting. You must, therefore, keep a reference to the OpenFile\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    during the life of the file-like it generates.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\fsspec\\core.py:105\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_magic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\fsspec\\spec.py:1303\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1302\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1303\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[0;32m   1304\u001b[0m         path,\n\u001b[0;32m   1305\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   1306\u001b[0m         block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1307\u001b[0m         autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[0;32m   1308\u001b[0m         cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1309\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1310\u001b[0m     )\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:191\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:355\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open()\n",
      "File \u001b[1;32mc:\\Users\\vbhav\\anaconda3\\Lib\\site-packages\\fsspec\\implementations\\local.py:360\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 360\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    362\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C://Users//vbhav//Downloads//prediction-data (1).csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "combined_job_listings = pd.read_csv('C://CUB//sem2//ml//proj//cn//datasets//combined_job_listings.csv')\n",
    "prediction_data = pd.read_csv('C://Users//vbhav//Downloads//prediction-data (1).csv')\n",
    "\n",
    "merged_data = pd.merge(combined_job_listings, prediction_data, left_on='title', right_on='SuggestedJobRole')\n",
    "\n",
    "merged_data.to_csv('merged_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data Rows: 703311\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "combined_job_listings = pd.read_csv('C://CUB//sem2//ml//proj//cn//datasets//combined_job_listings.csv')\n",
    "prediction_data = pd.read_csv('C://Users//vbhav//Downloads//prediction-data (1).csv')\n",
    "\n",
    "combined_job_listings['title'] = combined_job_listings['title'].str.strip()\n",
    "prediction_data['SuggestedJobRole'] = prediction_data['SuggestedJobRole'].str.strip()\n",
    "\n",
    "# Optional: Convert to the same case to ensure case-insensitive matching\n",
    "combined_job_listings['title'] = combined_job_listings['title'].str.lower()\n",
    "prediction_data['SuggestedJobRole'] = prediction_data['SuggestedJobRole'].str.lower()\n",
    "\n",
    "# Merge the datasets on the specified columns\n",
    "merged_data = pd.merge(combined_job_listings, prediction_data, left_on='title', right_on='SuggestedJobRole', how='inner')\n",
    "\n",
    "# Check the number of rows in the merged data\n",
    "print(f\"Merged Data Rows: {merged_data.shape[0]}\")\n",
    "if merged_data.shape[0] == 0:\n",
    "    print(\"No matches found. Please check the unique values of the columns used for merging.\")\n",
    "\n",
    "# Save the merged dataset to a new CSV file\n",
    "merged_data.to_csv('merged_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vbhav\\AppData\\Local\\Temp\\ipykernel_35176\\1221791975.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.rename(columns={'title': 'SuggestedJobRole'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(        SuggestedJobRole                                       requirements\n",
       " 0  Application Developer  Bachelor's degree in Computer Science, Informa...\n",
       " 1  Application Developer  Bachelor Degree in Computer Science or similar...\n",
       " 2  Application Developer  Minimum of 8 years experience with ServiceNow...\n",
       " 3  Application Developer  Building and deploying scalable machine learni...\n",
       " 4  Application Developer  Bachelors degree in Business, Sales, Marketin...,\n",
       " 'C://CUB//sem2//ml//proj//cn//datasets//filtered_job_listings.csv')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import necessary libraries due to state reset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the combined job listings CSV file\n",
    "combined_job_listings_path = 'C://CUB//sem2//ml//proj//cn//datasets//combined_job_listings.csv'\n",
    "combined_df = pd.read_csv(combined_job_listings_path)\n",
    "\n",
    "# Keep only 'title' and 'requirements' columns\n",
    "filtered_df = combined_df[['title', 'requirements']]\n",
    "\n",
    "# Change the column name 'title' to 'SuggestedJobRole'\n",
    "filtered_df.rename(columns={'title': 'SuggestedJobRole'}, inplace=True)\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "filtered_file_path = 'C://CUB//sem2//ml//proj//cn//datasets//filtered_job_listings.csv'\n",
    "filtered_df.to_csv(filtered_file_path, index=False)\n",
    "\n",
    "# Confirm the structure of the updated DataFrame\n",
    "filtered_df.head(), filtered_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's a column named 'description' or it's a mistake\n",
    "if 'description' in filtered_df.columns:\n",
    "    # If there's a 'description' column, we'll clean it\n",
    "    # Strip leading/trailing whitespace\n",
    "    filtered_df['description'] = filtered_df['description'].str.strip()\n",
    "    # Replace multiple spaces with a single space\n",
    "    filtered_df['description'] = filtered_df['description'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    # Optionally, we could also remove rows where 'description' is NaN if they exist\n",
    "    # filtered_df.dropna(subset=['description'], inplace=True)\n",
    "\n",
    "    # Confirm changes by displaying the updated data\n",
    "    filtered_df.head()\n",
    "else:\n",
    "    # If there's no such column, let's display a message\n",
    "    \"No 'description' column found. Available columns are: \" + \", \".join(filtered_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SuggestedJobRole</th>\n",
       "      <th>requirements</th>\n",
       "      <th>Logical quotient rating</th>\n",
       "      <th>hackathons</th>\n",
       "      <th>coding skills rating</th>\n",
       "      <th>public speaking points</th>\n",
       "      <th>self-learning capability?</th>\n",
       "      <th>Extra-courses did</th>\n",
       "      <th>certifications</th>\n",
       "      <th>workshops</th>\n",
       "      <th>...</th>\n",
       "      <th>memory capability score</th>\n",
       "      <th>Interested subjects</th>\n",
       "      <th>interested career area</th>\n",
       "      <th>Type of company want to settle in?</th>\n",
       "      <th>Taken inputs from seniors or elders</th>\n",
       "      <th>Interested Type of Books</th>\n",
       "      <th>Management or Technical</th>\n",
       "      <th>hard/smart worker</th>\n",
       "      <th>worked in teams ever?</th>\n",
       "      <th>Introvert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Application Developer</td>\n",
       "      <td>Bachelor's degree in Computer Science, Informa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Application Developer</td>\n",
       "      <td>Bachelor Degree in Computer Science or similar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Application Developer</td>\n",
       "      <td>Minimum of 8 years experience with ServiceNow...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Application Developer</td>\n",
       "      <td>Building and deploying scalable machine learni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Application Developer</td>\n",
       "      <td>Bachelors degree in Business, Sales, Marketin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SuggestedJobRole                                       requirements  \\\n",
       "0  Application Developer  Bachelor's degree in Computer Science, Informa...   \n",
       "1  Application Developer  Bachelor Degree in Computer Science or similar...   \n",
       "2  Application Developer  Minimum of 8 years experience with ServiceNow...   \n",
       "3  Application Developer  Building and deploying scalable machine learni...   \n",
       "4  Application Developer  Bachelors degree in Business, Sales, Marketin...   \n",
       "\n",
       "   Logical quotient rating  hackathons  coding skills rating  \\\n",
       "0                      NaN         NaN                   NaN   \n",
       "1                      NaN         NaN                   NaN   \n",
       "2                      NaN         NaN                   NaN   \n",
       "3                      NaN         NaN                   NaN   \n",
       "4                      NaN         NaN                   NaN   \n",
       "\n",
       "   public speaking points self-learning capability? Extra-courses did  \\\n",
       "0                     NaN                       NaN               NaN   \n",
       "1                     NaN                       NaN               NaN   \n",
       "2                     NaN                       NaN               NaN   \n",
       "3                     NaN                       NaN               NaN   \n",
       "4                     NaN                       NaN               NaN   \n",
       "\n",
       "  certifications workshops  ... memory capability score Interested subjects  \\\n",
       "0            NaN       NaN  ...                     NaN                 NaN   \n",
       "1            NaN       NaN  ...                     NaN                 NaN   \n",
       "2            NaN       NaN  ...                     NaN                 NaN   \n",
       "3            NaN       NaN  ...                     NaN                 NaN   \n",
       "4            NaN       NaN  ...                     NaN                 NaN   \n",
       "\n",
       "  interested career area  Type of company want to settle in?  \\\n",
       "0                     NaN                                NaN   \n",
       "1                     NaN                                NaN   \n",
       "2                     NaN                                NaN   \n",
       "3                     NaN                                NaN   \n",
       "4                     NaN                                NaN   \n",
       "\n",
       "  Taken inputs from seniors or elders Interested Type of Books  \\\n",
       "0                                 NaN                      NaN   \n",
       "1                                 NaN                      NaN   \n",
       "2                                 NaN                      NaN   \n",
       "3                                 NaN                      NaN   \n",
       "4                                 NaN                      NaN   \n",
       "\n",
       "  Management or Technical hard/smart worker worked in teams ever? Introvert  \n",
       "0                     NaN               NaN                   NaN       NaN  \n",
       "1                     NaN               NaN                   NaN       NaN  \n",
       "2                     NaN               NaN                   NaN       NaN  \n",
       "3                     NaN               NaN                   NaN       NaN  \n",
       "4                     NaN               NaN                   NaN       NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "filtered_listings_df = pd.read_csv('C://CUB//sem2//ml//proj//cn//datasets//filtered_job_listings.csv')\n",
    "prediction_data_df = pd.read_csv('C://Users//vbhav//Downloads//prediction-data (1).csv')\n",
    "\n",
    "# Perform a full outer join\n",
    "full_outer_joined_df = filtered_listings_df.merge(prediction_data_df, on='SuggestedJobRole', how='outer')\n",
    "\n",
    "# Save the result to a new CSV\n",
    "full_outer_joined_df.to_csv('C://CUB//sem2//ml//proj//cn//datasets//full_outer_joined_data.csv', index=False)\n",
    "\n",
    "# Display the first few rows to verify the join\n",
    "full_outer_joined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SuggestedJobRole                                           keywords\n",
      "0  Application Developer  engineering, bachelor, ma, ba, ms, science, co...\n",
      "1  Application Developer  bachelor, ma, ba, ms, business, science, compu...\n",
      "2  Application Developer                 ma, bachelor, ba, business, degree\n",
      "3  Application Developer                  ma, engineering, ba, ms, business\n",
      "4  Application Developer                 ma, bachelor, ba, business, degree\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Path to the CSV file on your local system\n",
    "path = 'C://CUB//sem2//ml//proj//cn//datasets//filtered_job_listings.csv'\n",
    "\n",
    "# Load the dataset\n",
    "job_listings_df = pd.read_csv(path)\n",
    "\n",
    "# Define a list of education-related keywords to extract\n",
    "keywords = [\n",
    "    'degree', 'bachelor', 'master', 'phd', 'diploma', \n",
    "    'computer science', 'engineering', 'business', 'economics', 'science', 'arts',\n",
    "    'mba', 'b.sc', 'm.sc', 'ba', 'ma', 'bs', 'ms'\n",
    "]\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\" Extract specific keywords from the text. \"\"\"\n",
    "    text = text.lower()\n",
    "    # Find all occurrences of each keyword\n",
    "    found_keywords = {word for word in keywords if word in text}\n",
    "    # Join keywords into a string separated by commas\n",
    "    return ', '.join(found_keywords)\n",
    "\n",
    "# Apply the keyword extraction function to the 'requirements' column\n",
    "job_listings_df['keywords'] = job_listings_df['requirements'].astype(str).apply(extract_keywords)\n",
    "\n",
    "# Drop the 'requirements' column\n",
    "job_listings_df.drop('requirements', axis=1, inplace=True)\n",
    "\n",
    "# Save the modified dataframe back to the same CSV file, overwriting the original\n",
    "job_listings_df.to_csv(path, index=False)\n",
    "\n",
    "# Optionally, print out the head of the dataframe to verify the changes\n",
    "print(job_listings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SuggestedJobRole                                           keywords  \\\n",
      "0  Application Developer  engineering, bachelor, ma, ba, ms, science, co...   \n",
      "1  Application Developer  bachelor, ma, ba, ms, business, science, compu...   \n",
      "2  Application Developer                 ma, bachelor, ba, business, degree   \n",
      "3  Application Developer                  ma, engineering, ba, ms, business   \n",
      "4  Application Developer                 ma, bachelor, ba, business, degree   \n",
      "\n",
      "   Logical quotient rating  hackathons  coding skills rating  \\\n",
      "0                      NaN         NaN                   NaN   \n",
      "1                      NaN         NaN                   NaN   \n",
      "2                      NaN         NaN                   NaN   \n",
      "3                      NaN         NaN                   NaN   \n",
      "4                      NaN         NaN                   NaN   \n",
      "\n",
      "   public speaking points self-learning capability? Extra-courses did  \\\n",
      "0                     NaN                       NaN               NaN   \n",
      "1                     NaN                       NaN               NaN   \n",
      "2                     NaN                       NaN               NaN   \n",
      "3                     NaN                       NaN               NaN   \n",
      "4                     NaN                       NaN               NaN   \n",
      "\n",
      "  certifications workshops  ... memory capability score Interested subjects  \\\n",
      "0            NaN       NaN  ...                     NaN                 NaN   \n",
      "1            NaN       NaN  ...                     NaN                 NaN   \n",
      "2            NaN       NaN  ...                     NaN                 NaN   \n",
      "3            NaN       NaN  ...                     NaN                 NaN   \n",
      "4            NaN       NaN  ...                     NaN                 NaN   \n",
      "\n",
      "  interested career area  Type of company want to settle in?  \\\n",
      "0                     NaN                                NaN   \n",
      "1                     NaN                                NaN   \n",
      "2                     NaN                                NaN   \n",
      "3                     NaN                                NaN   \n",
      "4                     NaN                                NaN   \n",
      "\n",
      "  Taken inputs from seniors or elders Interested Type of Books  \\\n",
      "0                                 NaN                      NaN   \n",
      "1                                 NaN                      NaN   \n",
      "2                                 NaN                      NaN   \n",
      "3                                 NaN                      NaN   \n",
      "4                                 NaN                      NaN   \n",
      "\n",
      "  Management or Technical hard/smart worker worked in teams ever? Introvert  \n",
      "0                     NaN               NaN                   NaN       NaN  \n",
      "1                     NaN               NaN                   NaN       NaN  \n",
      "2                     NaN               NaN                   NaN       NaN  \n",
      "3                     NaN               NaN                   NaN       NaN  \n",
      "4                     NaN               NaN                   NaN       NaN  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "filtered_listings_path = 'C://CUB//sem2//ml//proj//cn//datasets//filtered_job_listings.csv'\n",
    "prediction_data_path = 'C://CUB//sem2//ml//proj//cn//datasets//prediction-data.csv'\n",
    "\n",
    "filtered_listings_df = pd.read_csv(filtered_listings_path)\n",
    "prediction_data_df = pd.read_csv(prediction_data_path)\n",
    "\n",
    "# Perform a full outer join to include all records from both dataframes\n",
    "combined_df = filtered_listings_df.merge(prediction_data_df, on='SuggestedJobRole', how='outer')\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv('C://CUB//sem2//ml//proj//cn//datasets//combined1_job_listings.csv', index=False)\n",
    "\n",
    "# Optionally, print out the head of the combined dataframe to verify the changes\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SuggestedJobRole         keywords  Logical quotient rating  \\\n",
      "1398  CRM Technical Developer  ms, diploma, ma                      9.0   \n",
      "1399  CRM Technical Developer  ms, diploma, ma                      2.0   \n",
      "1400  CRM Technical Developer  ms, diploma, ma                      5.0   \n",
      "1401  CRM Technical Developer  ms, diploma, ma                      3.0   \n",
      "1402  CRM Technical Developer  ms, diploma, ma                      6.0   \n",
      "\n",
      "      hackathons  coding skills rating  public speaking points  \\\n",
      "1398         0.0                   5.0                     4.0   \n",
      "1399         2.0                   8.0                     6.0   \n",
      "1400         1.0                   6.0                     9.0   \n",
      "1401         5.0                   2.0                     8.0   \n",
      "1402         0.0                   3.0                     4.0   \n",
      "\n",
      "     self-learning capability? Extra-courses did   certifications  \\\n",
      "1398                        no               yes  app development   \n",
      "1399                        no               yes    distro making   \n",
      "1400                       yes               yes           python   \n",
      "1401                       yes               yes           hadoop   \n",
      "1402                        no                no       full stack   \n",
      "\n",
      "             workshops  ... memory capability score   Interested subjects  \\\n",
      "1398      data science  ...                    poor      data engineering   \n",
      "1399  game development  ...                    poor       cloud computing   \n",
      "1400  game development  ...               excellent  Software Engineering   \n",
      "1401   cloud computing  ...                    poor       cloud computing   \n",
      "1402   cloud computing  ...               excellent    parallel computing   \n",
      "\n",
      "       interested career area  Type of company want to settle in?  \\\n",
      "1398  Business process analyst                product development   \n",
      "1399          system developer                      Service Based   \n",
      "1400                   testing                                BPA   \n",
      "1401  Business process analyst                      Service Based   \n",
      "1402                  security  Testing and Maintainance Services   \n",
      "\n",
      "     Taken inputs from seniors or elders Interested Type of Books  \\\n",
      "1398                                  no    Religion-Spirituality   \n",
      "1399                                 yes                  Trilogy   \n",
      "1400                                 yes     Action and Adventure   \n",
      "1401                                 yes                Self help   \n",
      "1402                                 yes                Self help   \n",
      "\n",
      "     Management or Technical hard/smart worker worked in teams ever? Introvert  \n",
      "1398              Management       hard worker                   yes        no  \n",
      "1399               Technical      smart worker                   yes        no  \n",
      "1400               Technical       hard worker                    no       yes  \n",
      "1401               Technical       hard worker                    no       yes  \n",
      "1402              Management      smart worker                    no        no  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from your local system\n",
    "combined_df = pd.read_csv('C://CUB//sem2//ml//proj//cn//datasets//combined1_job_listings.csv')\n",
    "\n",
    "# Remove rows with any null values\n",
    "cleaned_df = combined_df.dropna()\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV file directly specifying the path\n",
    "cleaned_df.to_csv('C://CUB//sem2//ml//proj//cn//datasets//cleaned_combined1_job_listings.csv', index=False)\n",
    "\n",
    "# Optionally, print out the head of the cleaned dataframe to verify the changes\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SuggestedJobRole</th>\n",
       "      <th>keywords</th>\n",
       "      <th>Logical quotient rating</th>\n",
       "      <th>hackathons</th>\n",
       "      <th>coding skills rating</th>\n",
       "      <th>public speaking points</th>\n",
       "      <th>self-learning capability?</th>\n",
       "      <th>Extra-courses did</th>\n",
       "      <th>certifications</th>\n",
       "      <th>workshops</th>\n",
       "      <th>...</th>\n",
       "      <th>memory capability score</th>\n",
       "      <th>Interested subjects</th>\n",
       "      <th>interested career area</th>\n",
       "      <th>Type of company want to settle in?</th>\n",
       "      <th>Taken inputs from seniors or elders</th>\n",
       "      <th>Interested Type of Books</th>\n",
       "      <th>Management or Technical</th>\n",
       "      <th>hard/smart worker</th>\n",
       "      <th>worked in teams ever?</th>\n",
       "      <th>Introvert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>CRM Technical Developer</td>\n",
       "      <td>ms, diploma, ma</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>app development</td>\n",
       "      <td>data science</td>\n",
       "      <td>...</td>\n",
       "      <td>poor</td>\n",
       "      <td>data engineering</td>\n",
       "      <td>Business process analyst</td>\n",
       "      <td>product development</td>\n",
       "      <td>no</td>\n",
       "      <td>Religion-Spirituality</td>\n",
       "      <td>Management</td>\n",
       "      <td>hard worker</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>CRM Technical Developer</td>\n",
       "      <td>ms, diploma, ma</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>distro making</td>\n",
       "      <td>game development</td>\n",
       "      <td>...</td>\n",
       "      <td>poor</td>\n",
       "      <td>cloud computing</td>\n",
       "      <td>system developer</td>\n",
       "      <td>Service Based</td>\n",
       "      <td>yes</td>\n",
       "      <td>Trilogy</td>\n",
       "      <td>Technical</td>\n",
       "      <td>smart worker</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>CRM Technical Developer</td>\n",
       "      <td>ms, diploma, ma</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>python</td>\n",
       "      <td>game development</td>\n",
       "      <td>...</td>\n",
       "      <td>excellent</td>\n",
       "      <td>Software Engineering</td>\n",
       "      <td>testing</td>\n",
       "      <td>BPA</td>\n",
       "      <td>yes</td>\n",
       "      <td>Action and Adventure</td>\n",
       "      <td>Technical</td>\n",
       "      <td>hard worker</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>CRM Technical Developer</td>\n",
       "      <td>ms, diploma, ma</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>hadoop</td>\n",
       "      <td>cloud computing</td>\n",
       "      <td>...</td>\n",
       "      <td>poor</td>\n",
       "      <td>cloud computing</td>\n",
       "      <td>Business process analyst</td>\n",
       "      <td>Service Based</td>\n",
       "      <td>yes</td>\n",
       "      <td>Self help</td>\n",
       "      <td>Technical</td>\n",
       "      <td>hard worker</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>CRM Technical Developer</td>\n",
       "      <td>ms, diploma, ma</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>full stack</td>\n",
       "      <td>cloud computing</td>\n",
       "      <td>...</td>\n",
       "      <td>excellent</td>\n",
       "      <td>parallel computing</td>\n",
       "      <td>security</td>\n",
       "      <td>Testing and Maintainance Services</td>\n",
       "      <td>yes</td>\n",
       "      <td>Self help</td>\n",
       "      <td>Management</td>\n",
       "      <td>smart worker</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             SuggestedJobRole         keywords  Logical quotient rating  \\\n",
       "1398  CRM Technical Developer  ms, diploma, ma                      9.0   \n",
       "1399  CRM Technical Developer  ms, diploma, ma                      2.0   \n",
       "1400  CRM Technical Developer  ms, diploma, ma                      5.0   \n",
       "1401  CRM Technical Developer  ms, diploma, ma                      3.0   \n",
       "1402  CRM Technical Developer  ms, diploma, ma                      6.0   \n",
       "\n",
       "      hackathons  coding skills rating  public speaking points  \\\n",
       "1398         0.0                   5.0                     4.0   \n",
       "1399         2.0                   8.0                     6.0   \n",
       "1400         1.0                   6.0                     9.0   \n",
       "1401         5.0                   2.0                     8.0   \n",
       "1402         0.0                   3.0                     4.0   \n",
       "\n",
       "     self-learning capability? Extra-courses did   certifications  \\\n",
       "1398                        no               yes  app development   \n",
       "1399                        no               yes    distro making   \n",
       "1400                       yes               yes           python   \n",
       "1401                       yes               yes           hadoop   \n",
       "1402                        no                no       full stack   \n",
       "\n",
       "             workshops  ... memory capability score   Interested subjects  \\\n",
       "1398      data science  ...                    poor      data engineering   \n",
       "1399  game development  ...                    poor       cloud computing   \n",
       "1400  game development  ...               excellent  Software Engineering   \n",
       "1401   cloud computing  ...                    poor       cloud computing   \n",
       "1402   cloud computing  ...               excellent    parallel computing   \n",
       "\n",
       "       interested career area  Type of company want to settle in?  \\\n",
       "1398  Business process analyst                product development   \n",
       "1399          system developer                      Service Based   \n",
       "1400                   testing                                BPA   \n",
       "1401  Business process analyst                      Service Based   \n",
       "1402                  security  Testing and Maintainance Services   \n",
       "\n",
       "     Taken inputs from seniors or elders Interested Type of Books  \\\n",
       "1398                                  no    Religion-Spirituality   \n",
       "1399                                 yes                  Trilogy   \n",
       "1400                                 yes     Action and Adventure   \n",
       "1401                                 yes                Self help   \n",
       "1402                                 yes                Self help   \n",
       "\n",
       "     Management or Technical hard/smart worker worked in teams ever? Introvert  \n",
       "1398              Management       hard worker                   yes        no  \n",
       "1399               Technical      smart worker                   yes        no  \n",
       "1400               Technical       hard worker                    no       yes  \n",
       "1401               Technical       hard worker                    no       yes  \n",
       "1402              Management      smart worker                    no        no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
